import time
import random
import pandas as pd
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import re

#Importa la clase MeneameEntry para estructurar los datos de las noticias

from src.model.meneame_entry import MeneameEntry

#Inicializa el scraper con los par√°metros, max de 50 pag.y save_onterval, intervalo del que se guardan los datos cada 5 pag.

class MeneameScraper:
    def __init__(self, max_pages=50, save_interval=5):
        self.base_url = "https://meneame.net"  #URL base del sitio de Meneame.
        self.max_pages = max_pages  #N√∫mero m√°ximo de p√°ginas a scrapear
        self.save_interval = save_interval  #Intervalo de p√°ginas para guardar datos temporalmente.
        self.results = []  #Lista para almacenar los resultados de las noticias.
        self.failed_pages = []  #Lista para almacenar p√°ginas con errores.
      

     #Busca el contenedor de noticias con el id "newswrap".

    def scrape_page(self, page_number):
      
        Hace una solicitud HTTP a la p√°gina, procesa el contenido HTML y llama a `extract_news` para extraer las noticias.
        Si la respuesta no es exitosa (c√≥digo 200), retorna una lista vac√≠a.
        """
        url = f"{self.base_url}/?page={page_number}"  #Construye la URL de la p√°gina.
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}  # Cabecera para la solicitud HTTP
        
        print(f"\nüü¢ Scrapeando p√°gina {page_number}: {url}")  #Imprime mensaje indicando que se est√° scrapeando la p√°gina.
    
        response = requests.get(url, headers=headers)  #Realiza la solicitud HTTP GET.
        print(f"üîç Status code: {response.status_code}")  #Imprime el c√≥digo de estado de la respuesta.
    
        if response.status_code != 200:
            print(f"‚ùå Error en {url}: {response.status_code}")  #Si la respuesta no es exitosa, muestra un mensaje de error.
            return []  #Si la p√°gina no se puede cargar correctamente, retorna una lista vac√≠a.
    
#Funci√≥n de extracci√≥n de las noticias de una p√°gina.
    
    def extract_news(self, soup, page_number):
    #Buscamos el contenedor principal donde se encuentran las noticias.
    newswrap = soup.find(id="newswrap")
    
    #Si no se encuentra el contenedor 'newswrap', se marca la p√°gina como fallida.
    
    if not newswrap:
        print(f"‚ö†Ô∏è No se encontr√≥ 'newswrap' en la p√°gina {page_number}. Guardando p√°gina para revisi√≥n.")
        self.failed_pages.append(page_number)  #Agregamos la p√°gina a la lista de p√°ginas fallidas.
        return []  #Retornamos una lista vac√≠a ya que no se pudieron extraer noticias.

    #Buscamos todas las noticias dentro del contenedor 'newswrap', que tienen la clase 'news-summary'.
    
    news_summaries = newswrap.find_all(class_="news-summary")
    #Imprimimos el n√∫mero de noticias encontradas en la p√°gina.
    print(f"‚úÖ Noticias encontradas en p√°gina {page_number}: {len(news_summaries)}")
    
    
    #Si la p√°gina tiene menos de 25 noticias, la marcamos como fallida
    if len(news_summaries) < 25:
        print(f"‚ö†Ô∏è P√°gina {page_number} tiene menos de 25 noticias. Posible problema.")
        self.failed_pages.append(page_number)  # Marcamos la p√°gina como fallida
    results = []
    
    # Aqu√≠ continuar√≠as procesando cada resumen de noticia (lo que sigue es la l√≥gica de extracci√≥n de los detalles)

        for news_summary in news_summaries:
    try:      #Buscar el cuerpo de la noticia dentro del resumen.
        news_body = news_summary.find(class_="news-body")#Si no se encuentra el cuerpo, continuar con la siguiente noticia.
        if not news_body:
            continue
        news_id = int(news_body.get("data-link-id"))#Extraer el ID de la noticia.
        center_content = news_body.find_next(class_="center-content") #Buscar el contenido central de la noticia (t√≠tulo, enlace, e
        title = center_content.find("h2").find("a").text.strip() #Extraer el t√≠tulo de la noticia.
        source_link = center_content.find("h2").find("a")["href"]#Extraer el enlace fuente de la noticia.
        content_div = news_body.find("div", class_="news-content")#Extraer el contenido de la noticia, si est√° disponible.
        content = content_div.text.strip() if content_div else "" #Extracci√≥N y limpieza del texto.
        
        #Buscar la fecha de publicaci√≥n de la noticia.
        
        news_submitted = center_content.find("div", class_="news-submitted")
        published_timestamp = int(news_submitted.find_all("span", attrs={"data-ts": True})[-1].get("data-ts"))
        published_date = datetime.fromtimestamp(published_timestamp).strftime("%Y-%m-%d %H:%M:%S")
        
        #Extraer el nombre del usuario que public√≥ la noticia.
        
        user_link = news_submitted.find("a", href=re.compile("/user/.+/history"))
        user = user_link.text.strip() if user_link else "Desconocido"
        
        #Extraer la fuente de la noticia, si est√° disponible.
        
        source_span = news_submitted.find("span", class_="showmytitle")
        source = source_span.text.strip() if source_span else "Desconocido"
        
        #Buscar detalles adicionales como votos y comentarios.
        
        news_details = news_body.find_next(class_="news-details")
        comments = int(news_details.select_one("a.comments").get("data-comments-number"))
        positive_votes = int(news_details.select_one("span.positive-vote-number").text)
        anonymous_votes = int(news_details.select_one("span.anonymous-vote-number").text)
        negative_votes = int(news_details.select_one("span.negative-vote-number").text)
        karma = int(news_details.select_one("span.karma-number").text)
        category = news_details.select_one("a.subname").text.strip()
        
        #Extraer informaci√≥n de clicks y meneos
        
        clicks_span = news_body.find("span", id=f"clicks-number-{news_id}")
        clicks = int(clicks_span.text.strip()) if clicks_span else 0
        votes_a = news_body.find("a", id=f"a-votes-{news_id} ga-event")
        meneos = int(votes_a.text.strip()) if votes_a else 0
        
        #Obtener el enlace completo de la historia
        
        story_link = news_summary.find("a", href=re.compile("^/story/"))
        full_story_link = f"{self.base_url}{story_link['href']}" if story_link else "Desconocido"
        scraped_date = datetime.now().strftime("%Y-%m-%d") #Calculamos el tiempo total de ejecuci√≥n.
        
        #Crear un objeto MeneameEntry con toda la informaci√≥n de la noticia
        
        results.append(MeneameEntry(
            news_id, title, content, full_story_link, meneos, clicks, karma, positive_votes, anonymous_votes, negative_votes,
            category, comments, published_date, user, source, source_link, scraped_date
        ))
    
    except Exception as e:
        print(f"‚ö†Ô∏è Error procesando noticia en p√°gina {page_number}: {e}. Continuando con la siguiente noticia.")
        self.failed_pages.append(page_number)#imprime el error y agregar la p√°gina a la lista de fallidas.
        continue

#Devolver todos los resultados procesados.
return results

   def scrape_main_page(self, start_page=1):
    """Itera por las p√°ginas manualmente usando ?page=X."""
    start_time = time.time()  #Guardamos el tiempo de inicio para medir la duraci√≥n del scraping.

    #Iteramos por las p√°ginas desde la p√°gina inicial hasta la √∫ltima (self.max_pages).
       
    for page in range(start_page, self.max_pages + 1):
        try:
            new_data = self.scrape_page(page)#Llamamos a la funci√≥n scrape_page para obtener los datos de la p√°gina actual.
            self.results.extend(new_data)  #A√±adimos los datos obtenidos a la lista de resultado.
            if page % self.save_interval == 0:#Si la p√°gina es m√∫ltiplo de 'save_interval', guardamos los datos temporalmente.
                self.save_temp_data(page)
            sleep_time = random.uniform(1, 2)#Esperamos entre 1 y 2 segundos para evitar detecci√≥n por el sitio web.
            print(f"‚è≥ Esperando {sleep_time:.2f} segundos antes de la siguiente p√°gina...")
            time.sleep(sleep_time)  # Pausa la ejecuci√≥n por el tiempo calculado

        except Exception as e:
            print(f"‚ö†Ô∏è Error en la p√°gina {page}: {e}")#Si ocurre un error al procesar la p√°gina, lo mostramos y detenemos el scraping.
            break

       
    self.save_final_data() #Una vez terminado el scraping, guardamos los datos finales.
    self.save_failed_pages() #Tambi√©n guardamos las p√°ginas que tuvieron errores.  
    elapsed_time = time.time() - start_time #Calculamos el tiempo total de ejecuci√≥n.
    print(f"üèÅ Scraping finalizado en {elapsed_time:.2f} segundos.")


    #Funci√≥n para guardar los datos de self.results como un archivo CSV temporal con un nombre basado en el n√∫mero de p√°gina.
        
    def save_temp_data(self, page):
        """Guarda datos temporalmente cada X p√°ginas."""
        df_temp = pd.DataFrame([entry.to_dict() for entry in self.results])
        df_temp.to_csv(f"meneame_scraped_temp_{page}.csv", index=False, encoding="utf-8")
        print(f"üìÅ Datos guardados temporalmente en meneame_scraped_temp_{page}.csv")
        
        
    #La funci√≥n guarda los datos de self.results en un archivo CSV llamado meneame_scraped_final.csv.

    def save_final_data(self):
        """Guarda los datos finales en un CSV."""
        df = pd.DataFrame([entry.to_dict() for entry in self.results])
        df.to_csv("meneame_scraped_final.csv", index=False, encoding="utf-8")
        print("‚úÖ Datos guardados en meneame_scraped_final.csv")

    #Funci√≥n que guarda las p√°ginas que fallaron en un archivo csv, e imprimelas.

    def save_failed_pages(self):
        """Guarda en un archivo CSV las p√°ginas que tuvieron menos de 25 noticias o errores."""
        if self.failed_pages:
            df_failed = pd.DataFrame({"failed_pages": self.failed_pages})
            df_failed.to_csv("meneame_failed_pages.csv", index=False, encoding="utf-8")
            print(f"‚ö†Ô∏è P√°ginas problem√°ticas guardadas en meneame_failed_pages.csv")
        else:
            print("‚úÖ No se detectaron p√°ginas problem√°ticas.")


# Ejemplo para ejecutar el scraper
# scraper = MeneameScraper(max_pages=12000, save_interval=100)
# scraper.scrape_main_page(start_page=6194)
