import time
import random
import pandas as pd
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import re

from src.model.meneame_entry import MeneameEntry

# Diccionario de provincias y comunidades aut√≥nomas de Espa√±a
PROVINCIAS_COMUNIDADES = {
    "Madrid": "Comunidad de Madrid", "Barcelona": "Catalu√±a", "Valencia": "Comunidad Valenciana", "Sevilla": "Andaluc√≠a",
    "Zaragoza": "Arag√≥n", "M√°laga": "Andaluc√≠a", "Murcia": "Regi√≥n de Murcia", "Palma": "Islas Baleares",
    "Las Palmas": "Canarias", "Bilbao": "Pa√≠s Vasco", "Alicante": "Comunidad Valenciana", "C√≥rdoba": "Andaluc√≠a",
    "Valladolid": "Castilla y Le√≥n", "Vigo": "Galicia", "Gij√≥n": "Asturias", "Hospitalet": "Catalu√±a",
    "La Coru√±a": "Galicia", "Granada": "Andaluc√≠a", "Vitoria": "Pa√≠s Vasco", "Elche": "Comunidad Valenciana",
    "Oviedo": "Asturias", "Santa Cruz de Tenerife": "Canarias", "Badalona": "Catalu√±a", "Cartagena": "Murcia"
}


class MeneameScraper:
    def __init__(self, max_pages=50, save_interval=5):
        self.base_url = "https://meneame.net"
        self.max_pages = max_pages
        self.save_interval = save_interval
        self.results = []

    def scrape_page(self, page_number):
        """Scrapea una √∫nica p√°gina."""
        url = f"{self.base_url}/?page={page_number}"
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
        
        print(f"\nüü¢ Scrapeando p√°gina {page_number}: {url}")

        response = requests.get(url, headers=headers)
        print(f"üîç Status code: {response.status_code}")

        if response.status_code != 200:
            print(f"‚ùå Error en {url}: {response.status_code}")
            return []

        soup = BeautifulSoup(response.text, "lxml")
        return self.extract_news(soup)

    def extract_news(self, soup):
        """Extrae informaci√≥n de las noticias de una p√°gina."""
        newswrap = soup.find(id="newswrap")
        if not newswrap:
            print("‚ö†Ô∏è No se encontr√≥ 'newswrap'. Saltando p√°gina.")
            return []
    
        results = []
        news_summaries = newswrap.find_all(class_="news-summary")
        print(f"‚úÖ Noticias encontradas: {len(news_summaries)}")
    
        for news_summary in news_summaries:
            try:
                news_body = news_summary.find(class_="news-body")
                if not news_body:
                    continue
    
                news_id = int(news_body.get("data-link-id", 0))
    
                center_content = news_body.find_next(class_="center-content")
                title_tag = center_content.find("h2").find("a") if center_content else None
                title = title_tag.text.strip() if title_tag else "T√≠tulo no disponible"
    
                source_link = title_tag["href"] if title_tag else "Desconocido"
    
                content_div = news_body.find("div", class_="news-content")
                content = content_div.text.strip() if content_div else "Contenido no disponible"
    
                news_submitted = center_content.find("div", class_="news-submitted") if center_content else None
                published_timestamp = int(news_submitted.find_all("span", attrs={"data-ts": True})[-1].get("data-ts", 0)) if news_submitted else 0
                published_date = datetime.fromtimestamp(published_timestamp).strftime("%Y-%m-%d %H:%M:%S") if published_timestamp else "Fecha no disponible"
    
                author_link = news_submitted.find("a", href=re.compile("/user/.+/history")) if news_submitted else None
                author = author_link.text.strip() if author_link else "Desconocido"
    
                source_span = news_submitted.find("span", class_="showmytitle") if news_submitted else None
                source = source_span.text.strip() if source_span else "Desconocido"
    
                news_details = news_body.find_next(class_="news-details")
                comments = int(news_details.select_one("a.comments").get("data-comments-number", 0)) if news_details else 0
                positive_votes = int(news_details.select_one("span.positive-vote-number").text) if news_details else 0
                anonymous_votes = int(news_details.select_one("span.anonymous-vote-number").text) if news_details else 0
                negative_votes = int(news_details.select_one("span.negative-vote-number").text) if news_details else 0
                karma = int(news_details.select_one("span.karma-number").text) if news_details else 0
                category = news_details.select_one("a.subname").text.strip() if news_details else "Desconocido"
    
                clicks_span = news_body.find("span", id=f"clicks-number-{news_id}")
                clicks = int(clicks_span.text.strip()) if clicks_span else 0
    
                votes_a = news_body.find("a", id=f"a-votes-{news_id} ga-event")
                meneos = int(votes_a.text.strip()) if votes_a else 0
    
                scraped_date = datetime.now().strftime("%Y-%m-%d")
                provincia, comunidad = self.detect_province_region(title)
    
                results.append(MeneameEntry(
                    news_id, title, content, meneos, clicks, karma, positive_votes, anonymous_votes, negative_votes, category,
                    comments, published_date, author, source, source_link, provincia, comunidad, scraped_date
                ))
    
            except Exception as e:
                print(f"‚ö†Ô∏è Error procesando noticia: {e}")
    
        return results

    def detect_province_region(self, title):
        for provincia, comunidad in PROVINCIAS_COMUNIDADES.items():
            if provincia.lower() in title.lower():
                return provincia, comunidad
        return "Desconocido", "Desconocido"

    def scrape_main_page(self, start_page=1):
        """Itera por las p√°ginas manualmente usando ?page=X."""
        start_time = time.time()  # üî¥ Iniciar tiempo de ejecuci√≥n

        for page in range(start_page, self.max_pages + 1):
            try:
                new_data = self.scrape_page(page)
                self.results.extend(new_data)

                # Guardar cada X p√°ginas
                if page % self.save_interval == 0:
                    self.save_temp_data(page)

                # Espera entre 5 y 10 segundos para evitar detecci√≥n
                sleep_time = random.uniform(4, 6)
                print(f"‚è≥ Esperando {sleep_time:.2f} segundos antes de la siguiente p√°gina...")
                time.sleep(sleep_time)

            except Exception as e:
                print(f"‚ö†Ô∏è Error en la p√°gina {page}: {e}")
                break

        self.save_final_data()
        elapsed_time = time.time() - start_time
        print(f"üèÅ Scraping finalizado en {elapsed_time:.2f} segundos.")
        
    def save_temp_data(self, page):
        """Guarda datos temporalmente cada X p√°ginas."""
        df_temp = pd.DataFrame([entry.to_dict() for entry in self.results])
        df_temp.to_csv(f"meneame_scraped_temp_{page}.csv", index=False, encoding="utf-8")
        print(f"üìÅ Datos guardados temporalmente en meneame_scraped_temp_{page}.csv")
       
    def save_final_data(self):
        """Guarda los datos finales en un CSV."""
        df = pd.DataFrame([entry.to_dict() for entry in self.results])
        df.to_csv("meneame_scraped_final_2.csv", index=False, encoding="utf-8")
        print("‚úÖ Datos guardados en meneame_scraped_final_2.csv")


# Ejecutar el scraper
# scraper = MeneameScraper(max_pages=12000, save_interval=100)
# scraper.scrape_main_page(start_page=6194)