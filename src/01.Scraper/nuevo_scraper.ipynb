{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a7ec8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import glob\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "user = os.getenv(\"user\")\n",
    "sys.path.append(f\"/Users/{user}/Projects/Analisis-de-noticias/src\")\n",
    "\n",
    "from utils.text_processing import NewsProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09ec111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to scrape Meneame.net\n",
    "def scrape_meneame(max_pages=50, save_interval=50, last_scraped_date=None):\n",
    "    base_url = \"https://meneame.net\"\n",
    "    results = []\n",
    "\n",
    "    def scrape_page(page_number):\n",
    "        url = f\"{base_url}/?page={page_number}\"\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36',\n",
    "            'Accept-Language': 'es-ES,es;q=0.9',\n",
    "            'Referer': 'https://www.google.com'\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Error en {url}: {response.status_code}\")\n",
    "            return []\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"lxml\")\n",
    "        return extract_news(soup)\n",
    "\n",
    "    def extract_news(soup):\n",
    "        newswrap = soup.find(id=\"newswrap\")\n",
    "        if not newswrap:\n",
    "            return []\n",
    "\n",
    "        news_summaries = newswrap.find_all(class_=\"news-summary\")\n",
    "        new_entries = []\n",
    "\n",
    "        for news_summary in news_summaries:\n",
    "            try:\n",
    "                news_body = news_summary.find(class_=\"news-body\")\n",
    "                if not news_body:\n",
    "                    continue\n",
    "                \n",
    "                news_id = int(news_body.get(\"data-link-id\"))\n",
    "                center_content = news_body.find_next(class_=\"center-content\")\n",
    "                title = center_content.find(\"h2\").find(\"a\").text.strip()\n",
    "                source_link = center_content.find(\"h2\").find(\"a\")[\"href\"]\n",
    "\n",
    "                content_div = news_body.find(\"div\", class_=\"news-content\")\n",
    "                content = content_div.text.strip() if content_div else \"\"\n",
    "\n",
    "                news_submitted = center_content.find(\"div\", class_=\"news-submitted\")\n",
    "                published_timestamp = int(news_submitted.find_all(\"span\", attrs={\"data-ts\": True})[-1].get(\"data-ts\"))\n",
    "                published_date = datetime.fromtimestamp(published_timestamp).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                # Stop if we reach already scraped news\n",
    "                if last_scraped_date and published_date <= last_scraped_date:\n",
    "                    return \"STOP\"\n",
    "\n",
    "                user_link = news_submitted.find(\"a\", href=re.compile(\"/user/.+/history\"))\n",
    "                user = user_link.text.strip() if user_link else \"Desconocido\"\n",
    "\n",
    "                source_span = news_submitted.find(\"span\", class_=\"showmytitle\")\n",
    "                source = source_span.text.strip() if source_span else \"Desconocido\"\n",
    "\n",
    "                news_details = news_body.find_next(class_=\"news-details\")\n",
    "                comments = int(news_details.select_one(\"a.comments\").get(\"data-comments-number\"))\n",
    "                positive_votes = int(news_details.select_one(\"span.positive-vote-number\").text)\n",
    "                anonymous_votes = int(news_details.select_one(\"span.anonymous-vote-number\").text)\n",
    "                negative_votes = int(news_details.select_one(\"span.negative-vote-number\").text)\n",
    "                karma = int(news_details.select_one(\"span.karma-number\").text)\n",
    "                category = news_details.select_one(\"a.subname\").text.strip()\n",
    "\n",
    "                clicks_span = news_body.find(\"span\", id=f\"clicks-number-{news_id}\")\n",
    "                clicks = int(clicks_span.text.strip()) if clicks_span else 0\n",
    "                votes_a = news_body.find(\"a\", id=f\"a-votes-{news_id} ga-event\")\n",
    "                meneos = int(votes_a.text.strip()) if votes_a else 0\n",
    "\n",
    "                story_link = news_summary.find(\"a\", href=re.compile(\"^/story/\"))\n",
    "                full_story_link = f\"{base_url}{story_link['href']}\" if story_link else \"Desconocido\"\n",
    "\n",
    "                scraped_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                new_entries.append({\n",
    "                    \"news_id\": news_id, \"title\": title, \"content\": content, \"full_story_link\": full_story_link,\n",
    "                    \"meneos\": meneos, \"clicks\": clicks, \"karma\": karma, \"positive_votes\": positive_votes,\n",
    "                    \"anonymous_votes\": anonymous_votes, \"negative_votes\": negative_votes, \"category\": category,\n",
    "                    \"comments\": comments, \"published_date\": published_date, \"user\": user, \"source\": source,\n",
    "                    \"source_link\": source_link, \"scraped_date\": scraped_date\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error procesando noticia: {e}\")\n",
    "                continue\n",
    "\n",
    "        return new_entries\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        scraped_data = scrape_page(page)\n",
    "        \n",
    "        if scraped_data == \"STOP\":\n",
    "            break\n",
    "\n",
    "        results.extend(scraped_data)\n",
    "\n",
    "        time.sleep(random.uniform(1, 2))\n",
    "\n",
    "    if results:\n",
    "        save_new_data(results)\n",
    "        print(\"‚úÖ Nuevas noticias guardadas.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No hay nuevas noticias.\")\n",
    "\n",
    "# Get the next available filename\n",
    "def get_next_scraped_filename(directory=\"../00.data/scraped\", pattern=\"meneame_scraped_*.csv\"):\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    files = glob.glob(os.path.join(directory, pattern))\n",
    "\n",
    "    if not files:\n",
    "        return os.path.join(directory, \"meneame_scraped_1.csv\")\n",
    "    \n",
    "    existing_numbers = [int(re.search(r\"meneame_scraped_(\\d+)\\.csv\", f).group(1)) for f in files if re.search(r\"meneame_scraped_(\\d+)\\.csv\", f)]\n",
    "    next_number = max(existing_numbers) + 1 if existing_numbers else 1\n",
    "\n",
    "    return os.path.join(directory, f\"meneame_scraped_{next_number}.csv\")\n",
    "\n",
    "# Save only newly scraped data to a new file with the next number\n",
    "def save_new_data(new_data, directory=\"../00.data/scraped\"):\n",
    "    latest_file = get_latest_scraped_file(directory)\n",
    "\n",
    "    if latest_file and os.path.exists(latest_file):\n",
    "        existing_df = pd.read_csv(latest_file, encoding=\"utf-8\")\n",
    "\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "        new_rows = new_df[~new_df.apply(tuple, 1).isin(existing_df.apply(tuple, 1))]\n",
    "\n",
    "        if new_rows.empty:\n",
    "            print(\"‚ö†Ô∏è No new rows to save.\")\n",
    "            return\n",
    "    else:\n",
    "        new_rows = pd.DataFrame(new_data)\n",
    "\n",
    "    new_filename = get_next_scraped_filename(directory)\n",
    "    new_rows.to_csv(new_filename, index=False, encoding=\"utf-8\")\n",
    "    print(f\"üìÅ {len(new_rows)} new rows saved in {new_filename}\")\n",
    "\n",
    "# Get the most recent scraped file\n",
    "def get_latest_scraped_file(directory=\"../00.data/scraped\", pattern=\"meneame_scraped_*.csv\"):\n",
    "    files = glob.glob(os.path.join(directory, pattern))\n",
    "    return max(files, key=os.path.getmtime) if files else None\n",
    "\n",
    "# Get last scraped date\n",
    "def get_last_scraped_date(directory=\"../00.data/scraped\", pattern=\"meneame_scraped_*.csv\"):\n",
    "    latest_file = get_latest_scraped_file(directory)\n",
    "\n",
    "    if not latest_file:\n",
    "        return None  \n",
    "\n",
    "    df = pd.read_csv(latest_file, usecols=[\"scraped_date\"], encoding=\"utf-8\")\n",
    "    return df[\"scraped_date\"].max() if not df.empty else None\n",
    "\n",
    "# Scrape until the latest date\n",
    "def scrape_until_latest():\n",
    "    last_scraped_date = get_last_scraped_date()\n",
    "    print(f\"üìÖ √öltima fecha de scrapeo: {last_scraped_date}\")\n",
    "    \n",
    "    scrape_meneame(max_pages=50, save_interval=5, last_scraped_date=last_scraped_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59259815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ √öltima fecha de scrapeo: 2025-03-05 11:07:12\n",
      "üìÅ 125 new rows saved in ../00.data/scraped/meneame_scraped_6.csv\n",
      "‚úÖ Nuevas noticias guardadas.\n"
     ]
    }
   ],
   "source": [
    "scrape_until_latest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "640db9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_scrape = pd.read_csv(get_latest_scraped_file())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a4f5a4",
   "metadata": {},
   "source": [
    "## Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0fd4c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = NewsProcessor()\n",
    "\n",
    "new_scrape = new_scrape.drop_duplicates(subset=[\"news_id\"])\n",
    "\n",
    "new_scrape = processor.assign_province_and_community(new_scrape)\n",
    "\n",
    "new_scrape = processor.categorize_news(new_scrape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2d219a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load(\"../03_01.ML/random_forest_model.pkl\")\n",
    "vectorizer = joblib.load(\"../03_01.ML/tfidf_vectorizer.pkl\")\n",
    "label_encoder = joblib.load(\"../03_01.ML/label_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54db7065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x1/dfk_lr0j0pg5th31583lwbxc0000gn/T/ipykernel_31866/2159261483.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  meneame_unlabeled[\"category\"] = final_categories\n"
     ]
    }
   ],
   "source": [
    "#predecir categorias para instancias donde categoria = 'otros' con el modelo RFC_categorias\n",
    "\n",
    "meneame_labeled = new_scrape[new_scrape['category'] != \"Otros\"]\n",
    "\n",
    "meneame_unlabeled = new_scrape[new_scrape['category'] == \"Otros\"]\n",
    "\n",
    "X_unlabeled = vectorizer.transform(meneame_unlabeled[\"content\"])\n",
    "\n",
    "y_proba = model.predict_proba(X_unlabeled)\n",
    "\n",
    "max_probs = np.max(y_proba, axis=1)\n",
    "\n",
    "y_pred_indices = np.argmax(y_proba, axis=1)\n",
    "\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_indices)\n",
    "\n",
    "#si modelo esta <30% cierto sobre la categorisacion, lo clasifica como \"otros\"\n",
    "final_categories = np.where(max_probs >= 0.30, y_pred_labels, \"Otros\")\n",
    "\n",
    "meneame_unlabeled[\"category\"] = final_categories\n",
    "\n",
    "meneame_final = pd.concat([meneame_labeled, meneame_unlabeled], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccc34c4-48d4-4f31-86e4-f997d3081ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
