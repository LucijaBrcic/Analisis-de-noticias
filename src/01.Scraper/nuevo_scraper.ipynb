{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dda7bfb-8f8d-4d2e-a236-825ebdb30422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import glob\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec7da60-d5b0-4dbc-83dc-769b5214fac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_meneame(max_pages=50, save_interval=50, last_scraped_date=None):\n",
    "    base_url = \"https://meneame.net\"\n",
    "    results = []\n",
    "\n",
    "    def scrape_page(page_number):\n",
    "        url = f\"{base_url}/?page={page_number}\"\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36',\n",
    "            'Accept-Language': 'es-ES,es;q=0.9',\n",
    "            'Referer': 'https://www.google.com'\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Error en {url}: {response.status_code}\")\n",
    "            return []\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"lxml\")\n",
    "        return extract_news(soup)\n",
    "\n",
    "    def extract_news(soup):\n",
    "        newswrap = soup.find(id=\"newswrap\")\n",
    "        if not newswrap:\n",
    "            return []\n",
    "\n",
    "        news_summaries = newswrap.find_all(class_=\"news-summary\")\n",
    "        new_entries = []\n",
    "\n",
    "        for news_summary in news_summaries:\n",
    "            try:\n",
    "                news_body = news_summary.find(class_=\"news-body\")\n",
    "                if not news_body:\n",
    "                    continue\n",
    "                \n",
    "                news_id = int(news_body.get(\"data-link-id\"))\n",
    "                center_content = news_body.find_next(class_=\"center-content\")\n",
    "                title = center_content.find(\"h2\").find(\"a\").text.strip()\n",
    "                source_link = center_content.find(\"h2\").find(\"a\")[\"href\"]\n",
    "\n",
    "                content_div = news_body.find(\"div\", class_=\"news-content\")\n",
    "                content = content_div.text.strip() if content_div else \"\"\n",
    "\n",
    "                news_submitted = center_content.find(\"div\", class_=\"news-submitted\")\n",
    "                published_timestamp = int(news_submitted.find_all(\"span\", attrs={\"data-ts\": True})[-1].get(\"data-ts\"))\n",
    "                published_date = datetime.fromtimestamp(published_timestamp).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                if last_scraped_date and published_date <= last_scraped_date:\n",
    "                    return \"STOP\"\n",
    "\n",
    "                user_link = news_submitted.find(\"a\", href=re.compile(\"/user/.+/history\"))\n",
    "                user = user_link.text.strip() if user_link else \"Desconocido\"\n",
    "\n",
    "                source_span = news_submitted.find(\"span\", class_=\"showmytitle\")\n",
    "                source = source_span.text.strip() if source_span else \"Desconocido\"\n",
    "\n",
    "                news_details = news_body.find_next(class_=\"news-details\")\n",
    "                comments = int(news_details.select_one(\"a.comments\").get(\"data-comments-number\"))\n",
    "                positive_votes = int(news_details.select_one(\"span.positive-vote-number\").text)\n",
    "                anonymous_votes = int(news_details.select_one(\"span.anonymous-vote-number\").text)\n",
    "                negative_votes = int(news_details.select_one(\"span.negative-vote-number\").text)\n",
    "                karma = int(news_details.select_one(\"span.karma-number\").text)\n",
    "                category = news_details.select_one(\"a.subname\").text.strip()\n",
    "\n",
    "                clicks_span = news_body.find(\"span\", id=f\"clicks-number-{news_id}\")\n",
    "                clicks = int(clicks_span.text.strip()) if clicks_span else 0\n",
    "                votes_a = news_body.find(\"a\", id=f\"a-votes-{news_id} ga-event\")\n",
    "                meneos = int(votes_a.text.strip()) if votes_a else 0\n",
    "\n",
    "                story_link = news_summary.find(\"a\", href=re.compile(\"^/story/\"))\n",
    "                full_story_link = f\"{base_url}{story_link['href']}\" if story_link else \"Desconocido\"\n",
    "\n",
    "                scraped_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                new_entries.append({\n",
    "                    \"news_id\": news_id, \"title\": title, \"content\": content, \"full_story_link\": full_story_link,\n",
    "                    \"meneos\": meneos, \"clicks\": clicks, \"karma\": karma, \"positive_votes\": positive_votes,\n",
    "                    \"anonymous_votes\": anonymous_votes, \"negative_votes\": negative_votes, \"category\": category,\n",
    "                    \"comments\": comments, \"published_date\": published_date, \"user\": user, \"source\": source,\n",
    "                    \"source_link\": source_link, \"scraped_date\": scraped_date\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error procesando noticia: {e}\")\n",
    "                continue\n",
    "\n",
    "        return new_entries\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        scraped_data = scrape_page(page)\n",
    "        \n",
    "        if scraped_data == \"STOP\":\n",
    "            break\n",
    "\n",
    "        results.extend(scraped_data)\n",
    "\n",
    "        if page % save_interval == 0:\n",
    "            save_data(results, f\"meneame_scraped_temp_{page}.csv\")\n",
    "\n",
    "        time.sleep(random.uniform(1, 2))\n",
    "\n",
    "    if results:\n",
    "        save_data(results, \"meneame_scraped_4.csv\")\n",
    "        print(\"‚úÖ Nuevas noticias guardadas en meneame_scraped_4.csv\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No hay nuevas noticias.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a97b2cd-9771-4a21-bd87-96b05cc17b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = pd.read_csv(\"../00.data/scraped/meneame_scraped_1.csv\", encoding=\"utf-8\")\n",
    "# df2 = pd.read_csv(\"../00.data/scraped/meneame_scraped_2.csv\", encoding=\"utf-8\")\n",
    "# df3 = pd.read_csv(\"../00.data/scraped/meneame_scraped_3.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# df = pd.concat([df1,df2,df3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc9250f-da18-4866-9f19-a8aae221befa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_scraped_filename(directory=\"../00.data/scraped\", pattern=\"meneame_scraped_*.csv\"):\n",
    "    \"\"\"\n",
    "    Encuentra el siguiente nombre de archivo para guardar los datos scrapeados.\n",
    "    \"\"\"\n",
    "    files = glob.glob(os.path.join(directory, pattern))\n",
    "    \n",
    "    if not files:\n",
    "        return os.path.join(directory, \"meneame_scraped_1.csv\")\n",
    "    \n",
    "    # Extraer los n√∫meros de los archivos existentes\n",
    "    existing_numbers = []\n",
    "    for file in files:\n",
    "        match = re.search(r\"meneame_scraped_(\\d+)\\.csv\", file)\n",
    "        if match:\n",
    "            existing_numbers.append(int(match.group(1)))\n",
    "    \n",
    "    next_number = max(existing_numbers) + 1 if existing_numbers else 1\n",
    "    return os.path.join(directory, f\"meneame_scraped_{next_number}.csv\")\n",
    "\n",
    "def save_data(data, directory=\"../00.data/scraped\"):\n",
    "    \"\"\"\n",
    "    Guarda los datos scrapeados en un nuevo archivo numerado.\n",
    "    \"\"\"\n",
    "    os.makedirs(directory, exist_ok=True)  # Asegurar que el directorio existe\n",
    "    filename = get_next_scraped_filename(directory)\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False, encoding=\"utf-8\")\n",
    "    print(f\"üìÅ Datos guardados en {filename}\")\n",
    "\n",
    "def get_last_scraped_date(directory=\"../00.data/scraped\", pattern=\"meneame_scraped_*.csv\"):\n",
    "    # Buscar todos los archivos en el directorio que coincidan con el patr√≥n\n",
    "    files = glob.glob(os.path.join(directory, pattern))\n",
    "\n",
    "    if not files:\n",
    "        return None  # Si no hay archivos, no hay una √∫ltima fecha de scrapeo\n",
    "\n",
    "    # Leer solo la columna 'scraped_date' de cada archivo existente\n",
    "    dfs = [pd.read_csv(f, usecols=[\"scraped_date\"], encoding=\"utf-8\") for f in files if os.path.exists(f)]\n",
    "    \n",
    "    if not dfs:\n",
    "        return None  # Si no se pudieron leer archivos, no hay fecha de scrapeo\n",
    "\n",
    "    # Combinar los DataFrames y obtener la √∫ltima fecha de scrapeo\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    return df[\"scraped_date\"].max() if not df.empty else None\n",
    "\n",
    "def scrape_until_latest():\n",
    "    last_scraped_date = get_last_scraped_date()\n",
    "    print(f\"üìÖ √öltima fecha de scrapeo: {last_scraped_date}\")\n",
    "    \n",
    "    scrape_meneame(max_pages=50, save_interval=5, last_scraped_date=last_scraped_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb562ca6-cd34-47b0-b8da-ecc8a6dd458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_until_latest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccc34c4-48d4-4f31-86e4-f997d3081ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
