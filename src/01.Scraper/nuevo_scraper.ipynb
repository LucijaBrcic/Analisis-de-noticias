{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a7ec8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import glob\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to scrape Meneame.net\n",
    "def scrape_meneame(max_pages=50, save_interval=50, last_scraped_date=None):\n",
    "    base_url = \"https://meneame.net\"\n",
    "    results = []\n",
    "\n",
    "    def scrape_page(page_number):\n",
    "        url = f\"{base_url}/?page={page_number}\"\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36',\n",
    "            'Accept-Language': 'es-ES,es;q=0.9',\n",
    "            'Referer': 'https://www.google.com'\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Error en {url}: {response.status_code}\")\n",
    "            return []\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"lxml\")\n",
    "        return extract_news(soup)\n",
    "\n",
    "    def extract_news(soup):\n",
    "        newswrap = soup.find(id=\"newswrap\")\n",
    "        if not newswrap:\n",
    "            return []\n",
    "\n",
    "        news_summaries = newswrap.find_all(class_=\"news-summary\")\n",
    "        new_entries = []\n",
    "\n",
    "        for news_summary in news_summaries:\n",
    "            try:\n",
    "                news_body = news_summary.find(class_=\"news-body\")\n",
    "                if not news_body:\n",
    "                    continue\n",
    "                \n",
    "                news_id = int(news_body.get(\"data-link-id\"))\n",
    "                center_content = news_body.find_next(class_=\"center-content\")\n",
    "                title = center_content.find(\"h2\").find(\"a\").text.strip()\n",
    "                source_link = center_content.find(\"h2\").find(\"a\")[\"href\"]\n",
    "\n",
    "                content_div = news_body.find(\"div\", class_=\"news-content\")\n",
    "                content = content_div.text.strip() if content_div else \"\"\n",
    "\n",
    "                news_submitted = center_content.find(\"div\", class_=\"news-submitted\")\n",
    "                published_timestamp = int(news_submitted.find_all(\"span\", attrs={\"data-ts\": True})[-1].get(\"data-ts\"))\n",
    "                published_date = datetime.fromtimestamp(published_timestamp).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                # Stop if we reach already scraped news\n",
    "                if last_scraped_date and published_date <= last_scraped_date:\n",
    "                    return \"STOP\"\n",
    "\n",
    "                user_link = news_submitted.find(\"a\", href=re.compile(\"/user/.+/history\"))\n",
    "                user = user_link.text.strip() if user_link else \"Desconocido\"\n",
    "\n",
    "                source_span = news_submitted.find(\"span\", class_=\"showmytitle\")\n",
    "                source = source_span.text.strip() if source_span else \"Desconocido\"\n",
    "\n",
    "                news_details = news_body.find_next(class_=\"news-details\")\n",
    "                comments = int(news_details.select_one(\"a.comments\").get(\"data-comments-number\"))\n",
    "                positive_votes = int(news_details.select_one(\"span.positive-vote-number\").text)\n",
    "                anonymous_votes = int(news_details.select_one(\"span.anonymous-vote-number\").text)\n",
    "                negative_votes = int(news_details.select_one(\"span.negative-vote-number\").text)\n",
    "                karma = int(news_details.select_one(\"span.karma-number\").text)\n",
    "                category = news_details.select_one(\"a.subname\").text.strip()\n",
    "\n",
    "                clicks_span = news_body.find(\"span\", id=f\"clicks-number-{news_id}\")\n",
    "                clicks = int(clicks_span.text.strip()) if clicks_span else 0\n",
    "                votes_a = news_body.find(\"a\", id=f\"a-votes-{news_id} ga-event\")\n",
    "                meneos = int(votes_a.text.strip()) if votes_a else 0\n",
    "\n",
    "                story_link = news_summary.find(\"a\", href=re.compile(\"^/story/\"))\n",
    "                full_story_link = f\"{base_url}{story_link['href']}\" if story_link else \"Desconocido\"\n",
    "\n",
    "                scraped_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                new_entries.append({\n",
    "                    \"news_id\": news_id, \"title\": title, \"content\": content, \"full_story_link\": full_story_link,\n",
    "                    \"meneos\": meneos, \"clicks\": clicks, \"karma\": karma, \"positive_votes\": positive_votes,\n",
    "                    \"anonymous_votes\": anonymous_votes, \"negative_votes\": negative_votes, \"category\": category,\n",
    "                    \"comments\": comments, \"published_date\": published_date, \"user\": user, \"source\": source,\n",
    "                    \"source_link\": source_link, \"scraped_date\": scraped_date\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error procesando noticia: {e}\")\n",
    "                continue\n",
    "\n",
    "        return new_entries\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        scraped_data = scrape_page(page)\n",
    "        \n",
    "        if scraped_data == \"STOP\":\n",
    "            break\n",
    "\n",
    "        results.extend(scraped_data)\n",
    "\n",
    "        time.sleep(random.uniform(1, 2))\n",
    "\n",
    "    if results:\n",
    "        save_new_data(results)\n",
    "        print(\"‚úÖ Nuevas noticias guardadas.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No hay nuevas noticias.\")\n",
    "\n",
    "# Get the next available filename\n",
    "def get_next_scraped_filename(directory=\"../00.data/scraped\", pattern=\"meneame_scraped_*.csv\"):\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    files = glob.glob(os.path.join(directory, pattern))\n",
    "\n",
    "    if not files:\n",
    "        return os.path.join(directory, \"meneame_scraped_1.csv\")\n",
    "    \n",
    "    existing_numbers = [int(re.search(r\"meneame_scraped_(\\d+)\\.csv\", f).group(1)) for f in files if re.search(r\"meneame_scraped_(\\d+)\\.csv\", f)]\n",
    "    next_number = max(existing_numbers) + 1 if existing_numbers else 1\n",
    "\n",
    "    return os.path.join(directory, f\"meneame_scraped_{next_number}.csv\")\n",
    "\n",
    "# Save only newly scraped data to a new file with the next number\n",
    "def save_new_data(new_data, directory=\"../00.data/scraped\"):\n",
    "    latest_file = get_latest_scraped_file(directory)\n",
    "\n",
    "    if latest_file and os.path.exists(latest_file):\n",
    "        existing_df = pd.read_csv(latest_file, encoding=\"utf-8\")\n",
    "\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "        new_rows = new_df[~new_df.apply(tuple, 1).isin(existing_df.apply(tuple, 1))]\n",
    "\n",
    "        if new_rows.empty:\n",
    "            print(\"‚ö†Ô∏è No new rows to save.\")\n",
    "            return\n",
    "    else:\n",
    "        new_rows = pd.DataFrame(new_data)\n",
    "\n",
    "    new_filename = get_next_scraped_filename(directory)\n",
    "    new_rows.to_csv(new_filename, index=False, encoding=\"utf-8\")\n",
    "    print(f\"üìÅ {len(new_rows)} new rows saved in {new_filename}\")\n",
    "\n",
    "# Get the most recent scraped file\n",
    "def get_latest_scraped_file(directory=\"../00.data/scraped\", pattern=\"meneame_scraped_*.csv\"):\n",
    "    files = glob.glob(os.path.join(directory, pattern))\n",
    "    return max(files, key=os.path.getmtime) if files else None\n",
    "\n",
    "# Get last scraped date\n",
    "def get_last_scraped_date(directory=\"../00.data/scraped\", pattern=\"meneame_scraped_*.csv\"):\n",
    "    latest_file = get_latest_scraped_file(directory)\n",
    "\n",
    "    if not latest_file:\n",
    "        return None  \n",
    "\n",
    "    df = pd.read_csv(latest_file, usecols=[\"scraped_date\"], encoding=\"utf-8\")\n",
    "    return df[\"scraped_date\"].max() if not df.empty else None\n",
    "\n",
    "# Scrape until the latest date\n",
    "def scrape_until_latest():\n",
    "    last_scraped_date = get_last_scraped_date()\n",
    "    print(f\"üìÖ √öltima fecha de scrapeo: {last_scraped_date}\")\n",
    "    \n",
    "    scrape_meneame(max_pages=50, save_interval=5, last_scraped_date=last_scraped_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59259815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ √öltima fecha de scrapeo: 2025-03-04 14:12:57\n",
      "üìÅ 25 new rows saved in ../00.data/scraped/meneame_scraped_5.csv\n",
      "‚úÖ Nuevas noticias guardadas.\n"
     ]
    }
   ],
   "source": [
    "scrape_until_latest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fd4c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "funciones.scrape("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccc34c4-48d4-4f31-86e4-f997d3081ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
